# Vocab
VOCAB: MULTI_STR #[BASIC, BASIC_VELOCITY_BINS, MULTI_STR]

# Preprocessing parameters
Preprocessing:
  MAX_WORKERS: 2 # Max workers for tokenizer executor (based on num cpu cores)
  SAMPLE_SIZE: 178561 # Num files to preprocess
  BATCH_SIZE: 5000  # Batch size of preprocessing files
  TRAIN_RATIO: 0.80
  VAL_RATIO: 0.10
  TEST_RATIO: 0.10

# Model Params
MODEL:
  MODEL: 'V2' # DON'T USE V1
  BLOCK_SIZE: 1024
  N_EMBD: 1024
  N_HEAD: 8
  N_LAYER: 8
  INITIAL_DROPOUT: 0.3
  FINAL_DROPOUT: 0.1
  
# Training parameters
Training:
  EXPERIMENT_NAME: "100epoch_full_dataset" # name of experiment save dir
  BATCH_SIZE: 4
  ACCUMULATION_STEPS: 2
  NUM_EPOCHS: 100
  LEARNING_RATE: 1e-5

# Inference Parameters:
Inference:
  MODEL_NAME: "model_final.pt"
  OUTPUT_NAME: "paris_test_t0.85_tk355" # name of saved output file
  MAX_GENERATED_TOKENS: 2048  # maximum number of tokens to generate
  TEMPERATURE: 0.85
  TOP_K: 0 # set to 0 to disable top k

  SEED_PROMPT: [53, 380, 65, 380, 280, 181, 193, 261, 60, 380, 273, 188, 292, 68, 380, 56, 380, 280, 196, 184, 261, 60, 380, 273, 188, 298, 55, 380, 67, 380, 280, 183, 195, 261, 60, 380, 273, 188, 292, 53, 380, 65, 380, 280, 181, 193, 261, 60, 380, 273, 188, 261, 72, 380, 84, 380, 60, 380, 273, 200, 212, 188, 292, 53, 380, 65, 380, 280, 181, 193, 261, 60, 380, 273, 188, 292, 68, 380, 56, 380, 280, 196, 184, 261, 60, 380, 273, 188, 298, 55, 380, 67, 380, 280, 183, 195, 261, 60, 380, 273, 188, 292, 65, 380, 53, 380, 280, 193, 181, 261, 60, 380, 273, 188, 261, 72, 380, 84, 380, 60, 380, 273, 200, 212, 188]
